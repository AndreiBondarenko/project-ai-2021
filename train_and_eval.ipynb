{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mf import MatrixFactorization\n",
    "import scipy.sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(k: int, topk: np.ndarray, actual: scipy.sparse.csc_matrix):\n",
    "    return actual[:, topk].count_nonzero() / actual.count_nonzero()\n",
    "\n",
    "def precision_at_k(k: int, topk: np.ndarray, actual: scipy.sparse.csc_matrix):\n",
    "    return actual[:, topk].count_nonzero() / k\n",
    "\n",
    "def dcg(rel: np.ndarray):\n",
    "    if len(rel) < 1:\n",
    "        return 0\n",
    "    log2i = np.log2(np.asarray(range(1, len(rel) + 1)) + 1)\n",
    "    return ((np.power(2, rel) - 1) / log2i).sum()\n",
    "    \n",
    "def ndcg_at_k(k: int, topk: np.ndarray, actual: scipy.sparse.csc_matrix):\n",
    "    # retrieve relevant entries in topk. Non-relevant documents will get score 0.\n",
    "    rel = actual[:, topk].toarray()[0]\n",
    "    pad = max(0, k - len(rel))\n",
    "     # pad could be zero in which case this will no-op\n",
    "    rel = np.pad(rel, (0, pad), 'constant')\n",
    "    _dcg = dcg(rel)\n",
    "    _idcg = 0\n",
    "    rel.sort()\n",
    "    _idcg = dcg(rel[:-(k+1):-1])\n",
    "    _ndcg = 0\n",
    "    if _idcg > 0:\n",
    "        _ndcg = _dcg / _idcg\n",
    "    return _ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "x[:-122:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(_y[1, _y[1,:].nonzero()[1]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scipy.sparse.load_npz('./data/train.npz')\n",
    "y = scipy.sparse.load_npz('./data/test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = MatrixFactorization(K=100, iterations=200) # after 200-250 iterations, test error increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train, e_test = M.train(x, y)\n",
    "plt.plot(e_train, label=\"train\")\n",
    "plt.plot(e_test, label=\"test\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "M.save(\"checkpoint.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y = y.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = []\n",
    "for i in tqdm(range(y.shape[0])):\n",
    "    topk = M.recommend_sim(k=10, user=i)\n",
    "    actual = _y[i]\n",
    "    recall_scores.append(recall_at_k(k=10, topk=topk, actual=actual))\n",
    "    precision_scores.append(precision_at_k(k=10, topk=topk, actual=actual))\n",
    "    ndcg_scores.append(ndcg_at_k(k=10, topk=topk, actual=actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(sorted(recall_scores), label=\"recall\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'average recall: {np.mean(recall_scores)}')\n",
    "print(f'average precision: {np.mean(precision_scores)}')\n",
    "print(f'average ndcg: {np.mean(ndcg_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = []\n",
    "precision_scores = []\n",
    "ndcg_scores = []\n",
    "for i in tqdm(range(y.shape[0])):\n",
    "    topk = M.recommend(k=10, user=i)\n",
    "    actual = _y[i]\n",
    "    recall_scores.append(recall_at_k(k=10, topk=topk, actual=actual))\n",
    "    precision_scores.append(precision_at_k(k=10, topk=topk, actual=actual))\n",
    "    ndcg_scores.append(ndcg_at_k(k=10, topk=topk, actual=actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(recall_scores, label=\"recall\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'average recall: {np.mean(recall_scores)}')\n",
    "print(f'average precision: {np.mean(precision_scores)}')\n",
    "print(f'average ndcg: {np.mean(ndcg_scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-project-ai",
   "language": "python",
   "name": "venv-project-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
