{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we retrieve our dataset and preprocess it into a format that is ready to use for training our matrix factorization based recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "from utils import read_json_fast\n",
    "tqdm.pandas() # for progress_apply etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the files for our dataset (Goodreads). We use the gdown package to retrieve them from the Google Drive they're originally hosted on. \n",
    "\n",
    "> Since we will be implementing a collaborative filtering algorithm, we only need the interactions part of the dataset. The code for reading in the other parts of the dataset were left as comments for potential future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the URLs for each file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = {\n",
    "    # \"BOOKS\": \"https://drive.google.com/uc?id=1ICk5x0HXvXDp5Zt54CKPh5qz1HyUIn9m\",\n",
    "    # \"AUTHORS\": \"https://drive.google.com/uc?id=19cdwyXwfXx_HDIgxXaHzH0mrx8nMyLvC\",\n",
    "    # \"REVIEWS\": \"https://drive.google.com/u/0/uc?id=1V4MLeoEiPQdocCbUHjR_7L9ZmxTufPFe\",\n",
    "    \"INTERACTIONS\": \"https://drive.google.com/uc?id=1CCj-cQw_mJLMdvF_YYfQ7ibKA-dC_GA2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and download each file. (if they haven't been downloaded in a previous run of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: ./data/INTERACTIONS.json.gz\n"
     ]
    }
   ],
   "source": [
    "for name, url in URLS.items():\n",
    "    gdown.cached_download(url, f\"./data/{name}.json.gz\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the dataset file locations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_file = './data/BOOKS.json.gz' # book metadata\n",
    "interactions_file = './data/INTERACTIONS.json.gz' # user-book interactions (ratings)\n",
    "# reviews_file = './data/REVIEWS.json.gz' # user-book interactions (reviews)\n",
    "# authors_file = './data/AUTHORS.json.gz' # author metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and load the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing INTERACTIONS.json.gz:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fc94a8686d4226b6748911c9078186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0lines [00:00, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_books = read_json_fast(books_file)\n",
    "df_interactions = read_json_fast(interactions_file)\n",
    "# df_authors =  read_json_fast(authors_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the contents of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>is_read</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text_incomplete</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>836610</td>\n",
       "      <td>6b4db26aafeaf0da77c7de6214331e1e</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Mon Aug 21 12:11:00 -0700 2017</td>\n",
       "      <td>Mon Aug 21 12:11:00 -0700 2017</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id book_id                         review_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  836610  6b4db26aafeaf0da77c7de6214331e1e   \n",
       "\n",
       "   is_read  rating review_text_incomplete                      date_added  \\\n",
       "0    False       0                         Mon Aug 21 12:11:00 -0700 2017   \n",
       "\n",
       "                     date_updated read_at started_at  \n",
       "0  Mon Aug 21 12:11:00 -0700 2017                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_interactions.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset contains quite a few columns that are of no use to us. To make everything a little less cluttered we remove the columns that we don't use from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions = df_interactions[['user_id', 'book_id', 'rating', 'date_updated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>836610</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon Aug 21 12:11:00 -0700 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id book_id  rating  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  836610       0   \n",
       "\n",
       "                     date_updated  \n",
       "0  Mon Aug 21 12:11:00 -0700 2017  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_interactions.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreed pre-processing, training and evaluation for the goodreads team (after discussion with Len)\n",
    "\n",
    "#### preprocessing\n",
    "\n",
    "- min. sup. users = 5\n",
    "- min sup items = 1 (should be the case in dataset already, check to be sure)\n",
    "- keep all the ratings and scores\n",
    "\n",
    "#### training\n",
    "\n",
    "- For validation we consider doing 5 random 80%/20% train-test splits.\n",
    "- For each train-test pair, we first perform hyperparameter optimisation on the train part (via cross-validation) and then evaluate on the test part.\n",
    "- This gives us 5x(recall@10, ndcg@10) for which we compute the mean and stdev.\n",
    "- --> for the 5 random splits we should all use the same seeds\n",
    "\n",
    "#### evaluation\n",
    "\n",
    "- recall k = 5, 10\n",
    "- NDCG = 5, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pre-processing step we apply is converting all dates into a more standardized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c27fe97f4ba43a4bc1d530c64326a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7347630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:9: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "format_str = '%a %b %d %H:%M:%S %z %Y' #see https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n",
    "def convert_date(date_string):\n",
    "  return pd.to_datetime(date_string, utc=True, format=format_str)\n",
    "\n",
    "_df_interactions = df_interactions.copy()\n",
    "# _df_interactions['date_updated'] =  _df_interactions['date_updated'].progress_apply(convert_date)\n",
    "_df_interactions['date_updated'] = _df_interactions['date_updated'].progress_apply(lambda s: np.datetime64(datetime.strptime(s,format_str)))\n",
    "_df_interactions['date_updated'] = _df_interactions['date_updated'].dt.tz_localize(None)  # drops utc timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a pre-processing function that:\n",
    "\n",
    "1. Keeps all ratings, including the zero-ratings\n",
    "2. Removes duplicate (user, item) pairs.\n",
    "3. Removes users that occur in less than minsup interactions.\n",
    "4. Removes items that occur in less than 1 interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optimization phase:\n",
    "# Default minsup for users changed from 10 to 5\n",
    "# Default minsup for items is added and defaults to 1\n",
    "# All scores are taken into account because implicit feedback is used\n",
    "def preprocess(df, user_minsup=5, item_minsup=1, min_score=None):\n",
    "    \"\"\"\n",
    "    Goal: - Remove reconsumption items\n",
    "          - Remove users that have less than user_minsup interactions \n",
    "          - Remove items that have less than item_minsup interactions\n",
    "               \n",
    "    :input df: Dataframe containing user_id, item_id and time\n",
    "    \"\"\"\n",
    "    # drop 0 ratings\n",
    "    if min_score is not None:\n",
    "        before = df.shape[0]\n",
    "        df = df[(df[\"rating\"] >= min_score)]\n",
    "        print(f\"After dropping ratings below {min_score}: {before} -> {df.shape[0]}\")\n",
    "    # drop reconsumption items\n",
    "    before = df.shape[0]\n",
    "    df = df.drop_duplicates(subset=[\"user_id\",\"book_id\"])\n",
    "    print(f\"After drop_duplicates (reconsumption items): {before} -> {df.shape[0]}\")\n",
    "    # drop users with less than user_minsup items in history\n",
    "    if user_minsup is not None:\n",
    "        before = df.shape[0]\n",
    "        g = df.groupby('user_id', as_index=False)['book_id'].size()\n",
    "        g = g.rename({'size': 'user_sup'}, axis='columns')\n",
    "        g = g[g.user_sup >= user_minsup]\n",
    "        df = pd.merge(df, g, how='inner', on=['user_id'])\n",
    "        print(f\"After dropping users with less than {user_minsup} interactions: {before} -> {df.shape[0]}\")\n",
    "    # drop items with less than item_minsup items in history\n",
    "    if item_minsup is not None:\n",
    "        before = df.shape[0]\n",
    "        g = df.groupby('book_id', as_index=False)['book_id'].size()\n",
    "        g = g.rename({'size': 'item_sup'}, axis='columns')\n",
    "        g = g[g.item_sup >= item_minsup]\n",
    "        df = pd.merge(df, g, how='inner', on=['book_id'])\n",
    "        print(f\"After dropping items with less than {item_minsup} interactions: {before} -> {df.shape[0]}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique users: 342415\n",
      "number of unique items: 89411\n",
      "After drop_duplicates (reconsumption items): 7347630 -> 7347630\n",
      "After dropping users with less than 5 interactions: 7347630 -> 6995891\n",
      "After dropping items with less than 1 interactions: 6995891 -> 6995891\n",
      "number of unique users: 148438\n",
      "number of unique items: 89276\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>user_sup</th>\n",
       "      <th>item_sup</th>\n",
       "      <th>user_id_seq</th>\n",
       "      <th>book_id_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>836610</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-21 19:11:00</td>\n",
       "      <td>14</td>\n",
       "      <td>2222</td>\n",
       "      <td>79093</td>\n",
       "      <td>84684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37e4d1438f5918fd1400c12b49b80f61</td>\n",
       "      <td>836610</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-06 04:19:55</td>\n",
       "      <td>261</td>\n",
       "      <td>2222</td>\n",
       "      <td>32344</td>\n",
       "      <td>84684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3d0b73b0f3580b60075b04ea76dd4cd</td>\n",
       "      <td>836610</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-02-16 03:36:35</td>\n",
       "      <td>93</td>\n",
       "      <td>2222</td>\n",
       "      <td>95123</td>\n",
       "      <td>84684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5752e83b5c1ccdd8014aaf358b80c199</td>\n",
       "      <td>836610</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-08-11 18:11:43</td>\n",
       "      <td>46</td>\n",
       "      <td>2222</td>\n",
       "      <td>50661</td>\n",
       "      <td>84684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eadf8c06370d9c6fd74121723c8d20e3</td>\n",
       "      <td>836610</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-04-13 03:23:31</td>\n",
       "      <td>736</td>\n",
       "      <td>2222</td>\n",
       "      <td>136236</td>\n",
       "      <td>84684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id book_id  rating        date_updated  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  836610       0 2017-08-21 19:11:00   \n",
       "1  37e4d1438f5918fd1400c12b49b80f61  836610       5 2012-06-06 04:19:55   \n",
       "2  a3d0b73b0f3580b60075b04ea76dd4cd  836610       5 2016-02-16 03:36:35   \n",
       "3  5752e83b5c1ccdd8014aaf358b80c199  836610       4 2009-08-11 18:11:43   \n",
       "4  eadf8c06370d9c6fd74121723c8d20e3  836610       5 2014-04-13 03:23:31   \n",
       "\n",
       "   user_sup  item_sup  user_id_seq  book_id_seq  \n",
       "0        14      2222        79093        84684  \n",
       "1       261      2222        32344        84684  \n",
       "2        93      2222        95123        84684  \n",
       "3        46      2222        50661        84684  \n",
       "4       736      2222       136236        84684  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print number of users and items\n",
    "print(f\"number of unique users: {_df_interactions['user_id'].nunique()}\")\n",
    "print(f\"number of unique items: {_df_interactions['book_id'].nunique()}\")\n",
    "processed_df_interactions = preprocess(_df_interactions.copy())\n",
    "# display(processed_df_interactions.head(5))\n",
    "print(f\"number of unique users: {processed_df_interactions['user_id'].nunique()}\")\n",
    "print(f\"number of unique items: {processed_df_interactions['book_id'].nunique()}\")\n",
    "# create sequential ids\n",
    "processed_df_interactions['user_id_seq'] = processed_df_interactions['user_id'].astype('category').cat.codes\n",
    "processed_df_interactions['book_id_seq'] = processed_df_interactions['book_id'].astype('category').cat.codes\n",
    "# merge book id and rating for easier \n",
    "display(processed_df_interactions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the pre-processing function to the dataframe and log the change in number of samples, number of unique users and number of unique items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the mapping from user/book id's to their sequential id in an external file. This might come in handy in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_interactions[['user_id', 'user_id_seq']].drop_duplicates().to_pickle(\"./data/user_id_map.pkl\")\n",
    "processed_df_interactions[['book_id', 'book_id_seq']].drop_duplicates().to_pickle(\"./data/book_id_map.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the interactions by their date and group them by user. This allows us to perform a session-based train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform session-based split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_session(df, perc_train=0.7):\n",
    "    # Sort on date and group per user\n",
    "    sessions_df = df.sort_values(['date_updated'],ascending=True).groupby(by='user_id_seq', as_index=False)[['book_id_seq', 'rating', 'date_updated']].agg(list)\n",
    "\n",
    "    # Function to perform split\n",
    "    def split(row, col, percentage_train):\n",
    "        items = row[col]\n",
    "        no_train_items = math.floor(len(items) * percentage_train)\n",
    "        return items[0:no_train_items], items[no_train_items:]\n",
    "\n",
    "    # Default: split dataset into 0.7 training and 0.3 test samples, split in the temporal dimension.\n",
    "    percentage_train = perc_train\n",
    "    # train_items, test_items = split(items, percentage_train)\n",
    "    sessions_df[['history', 'future']] = sessions_df.progress_apply(lambda row: split(row, 'book_id_seq', percentage_train), axis=1, result_type='expand')\n",
    "    sessions_df[['history_ratings', 'future_ratings']] = sessions_df.progress_apply(lambda row: split(row, 'rating', percentage_train), axis=1, result_type='expand')\n",
    "    return sessions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a sparse representation of the user-item interaction matrix for our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_repr_session(df, column, shape):\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        items = row[column]\n",
    "        item_ids.extend(items)\n",
    "        user = row['user_id_seq']\n",
    "        user_ids.extend([user] * len(items))\n",
    "        ratings = row[column + \"_ratings\"]\n",
    "        values.extend(ratings)\n",
    "    matrix = scipy.sparse.coo_matrix((values, (user_ids, item_ids)), shape=shape, dtype=np.int32)\n",
    "    return matrix\n",
    "\n",
    "def train_test_split_coo(df):\n",
    "    shape = (df['user_id_seq'].max() + 1,  df['book_id_seq'].max() + 1)\n",
    "    session_df = train_test_split_session(df.copy())\n",
    "    # display(session_df.head(5))\n",
    "    train_coo = create_sparse_repr_session(session_df, column='history', shape=shape)\n",
    "    test_coo = create_sparse_repr_session(session_df, column='future', shape=shape)\n",
    "    return train_coo, test_coo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the train and test set externally to be used in the training and evaluating notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cdcf7a205848389cb22045998bf06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0df3beeeff94a8082917e9856ee40f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414d7c202105440fac57205e5df0c11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3331fce13b34a52b278cc25e9d092de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = train_test_split_coo(processed_df_interactions)\n",
    "scipy.sparse.save_npz('./data/train.npz', train)\n",
    "scipy.sparse.save_npz('./data/test.npz', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit feedback processing\n",
    "\n",
    "The user's raging score for a book ranges from \"1\" to \"5'\", \"0\" indicates \"not provided\"\n",
    "We convert the explicit feedback into implicit feedback using following rules:\n",
    "P: preference, C: confidence, alpha: rate of increase (default 40)\n",
    "\n",
    "| R         | P             | C            |\n",
    "| ---------:|--------------:|-------------:|\n",
    "| NA       | 0            | 1 |\n",
    "| 0        | 0            | 2 |\n",
    "| < 2.5    | 0            | 1 + (alpha x (5 - R)) |\n",
    "| > 2.5    | 1            | 1 + (alpha x R) |\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparce_cui_matrix(df, alpha=40):\n",
    "    \"\"\"\n",
    "    Creates a user-item confidence sparse matrix \n",
    "    \n",
    "    The user-item matrix does double duty here. It defines which items are liked by which\n",
    "    users (P_iu in the original paper), as well as how much confidence we have that the user\n",
    "    liked the item (C_iu).\n",
    "    The positive items (rating > 2.5) are defined with a positive C_ui, \n",
    "    the negative items (rating < 2.5 ) are defined with a negative C_ui.\n",
    "    Unseen items (rating 0) are defined with negative C_ui = -2.\n",
    "    items without a rating are implicitly defined with C_ui = -1 but are not stored in the matrix.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.Dataframe): user-item-scores data frame\n",
    "        alpha (int): rate of increase\n",
    "        \n",
    "    Returns:\n",
    "        cui_matrix (scp.sparse.coo): user-item confidence sparse matrix\n",
    "    \"\"\"\n",
    "    df_cui = df.copy()[['user_id_seq', 'book_id_seq', 'rating']]\n",
    "    # df_cui.loc[:, 'cui'] = -2 # default cui for non-seen items\n",
    "    df_cui = df_cui[df_cui.rating > 2]\n",
    "    df_cui.loc[:, 'cui'] = 1 + alpha * (df_cui.rating - 2)\n",
    "    # df_cui.loc[mask, 'cui'] = 1 + alpha * (df.rating - 2)\n",
    "    # mask = ((df.rating < 2.5) & (df.rating > 0))\n",
    "    # df_cui.loc[mask, 'cui'] = -(1 + alpha * (3 - df.rating))\n",
    "    df_cui = df_cui.drop(columns='rating')\n",
    "    shape = (df['user_id_seq'].max() + 1,  df['book_id_seq'].max() + 1)\n",
    "    matrix = scipy.sparse.coo_matrix((df_cui.cui, (df_cui.user_id_seq, df_cui.book_id_seq)), shape=shape, dtype=np.float64)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_rating = processed_df_interactions[['user_id_seq', 'book_id_seq', 'rating', 'date_updated']]\n",
    "cui = create_sparce_cui_matrix(df_rating)\n",
    "scipy.sparse.save_npz('./data/cui2.npz', cui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cui.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-project-ai",
   "language": "python",
   "name": "venv-project-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
